{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1XNpq0VCC_cJJWFMHid2YbGsKznDv3mc1","authorship_tag":"ABX9TyNAf5bFxo9FbtmY2HmUhiHh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Vision Transformer (ViT-Tiny) Fine-tuning 예제\n","\n","이 노트북에서는 **입력 크기 33×33**인 MC 히트맵 이미지를 **Vision Transformer (ViT-Tiny)** 모델로 분류하는 과정을 단계별로 다룹니다.\n","\n","**이 예제의 목표**\n","1. YAML 설정 파일을 활용해 하이퍼파라미터를 관리한다.\n","2. `torch.utils.data.Dataset`을 활용해 JetHeatmapDataset을 구현한다.\n","3. 사전학습(pretrained) ViT-Tiny 모델을 로드하고, 블록 동결(partial fine-tuning) 옵션을 적용한다.\n","4. 학습/검증 루프를 직접 구현하여 Loss와 Accuracy를 모니터링한다.\n","5. 최종 모델을 저장하고, 새로운 데이터에 대해 추론하는 방법을 확인한다.\n","\n","각 단계마다 **코드 셀**을 단계별로 구성하였으며, 실행 흐름을 따라가며 ViT fine-tuning 파이프라인을 한눈에 파악해 보세요."],"metadata":{"id":"hzVS5keQQSd_"}},{"cell_type":"markdown","source":["### vit-tiny.yaml 생성 스크립트\n","\n","- **용도**  \n","  ImageNet 사전학습(pretrained)된 ViT-Tiny 모델을 불러와 일부 블록만 fine-tuning할 때 사용\n","\n","- **주요 설정**  \n","  - `model.embed_dim = 192`, `model.depth = 12` (원본 ViT-Tiny 구조)  \n","  - `model.finetune_type = \"full\"` → 전체 블록 학습  \n","  - `model.finetune_type = \"partial\"` → 뒤쪽 `model.num_trainable_blocks = 2` 블록만 학습  \n","  - `optimizer.name = \"AdamW\"`, `optimizer.lr = 1e-3`, `optimizer.weight_decay = 0.01`  \n","  - `scheduler.name = \"Linear\"` with warmup 2 에포크  \n","  - `transform.resize = True`, `transform.normalize = True` (mean/std = 0.5)\n","\n","- **사용 방법**  \n","  1. 스크립트를 실행하면 `/content/drive/MyDrive/예제/vit-tiny.yaml` 파일이 생성됨  \n","  2. 노트북 실행 전 해당 YAML을 변경하지 말 것  \n","  3. `model.finetune_type`만 “full” 또는 “partial”로 변경하여 학습 블록 수 조절 가능  "],"metadata":{"id":"ZGMgCa3xh8ej"}},{"cell_type":"code","source":["import yaml\n","from pathlib import Path\n","\n","# 1) 프로젝트 루트 경로\n","BASE = Path(r\"/content/drive/MyDrive/예제\")\n","\n","\n","# 2) yaml에 저장할 하이퍼파라미터 설정\n","vit_tiny_cfg = {\n","    \"epochs\": 10,\n","    \"batch_size\": 128,\n","    \"num_workers\": 0,\n","\n","    \"model\": {\n","        \"name\": \"vit_tiny_patch16_224\",\n","        \"patch_size\": 16,\n","        \"img_size\": 224,\n","        \"embed_dim\": 192,\n","        \"depth\": 12,\n","        \"num_heads\": 2,\n","        \"mlp_ratio\": 4.0,\n","        \"qkv_bias\": True,\n","        \"drop_rate\": 0.1,\n","        \"attn_drop_rate\": 0.1,\n","        \"drop_path_rate\": 0.1,\n","        \"num_classes\": 2,\n","        \"finetune_type\": \"full\",       # \"full\" 또는 \"partial\"\n","        \"num_trainable_blocks\": 2\n","    },\n","\n","    \"optimizer\": {\n","        \"name\": \"AdamW\",\n","        \"lr\": 1e-3,\n","        \"weight_decay\": 0.01\n","    },\n","\n","    \"scheduler\": {\n","        \"name\": \"Linear\",                # 예: \"StepLR\", \"Cosine\", \"Linear\"\n","        \"total_epochs\": 10,\n","        \"warmup_epochs\": 2\n","    },\n","\n","    \"transform\": {\n","        \"resize\": True,\n","        \"random_rotation\": 0,\n","        \"horizontal_flip\": False,\n","        \"vertical_flip\": False,\n","        \"normalize\": True,\n","        \"mean\": [0.5, 0.5, 0.5],\n","        \"std\":  [0.5, 0.5, 0.5]\n","    },\n","\n","}\n","\n","# 3) YAML 파일 저장\n","outfile = BASE / \"vit-tiny.yaml\"\n","with open(outfile, \"w\", encoding=\"utf-8\") as f:\n","    yaml.dump(vit_tiny_cfg, f, default_flow_style=False, sort_keys=False)\n","\n","print(f\"✅ YAML 설정 파일 저장 완료: {outfile}\")"],"metadata":{"id":"c3lcrRd0hkx5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751038314905,"user_tz":-540,"elapsed":46,"user":{"displayName":"김다은","userId":"00408292954028336368"}},"outputId":"3f846c32-69cb-4c9a-f787-2536f2d94e32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ YAML 설정 파일 저장 완료: /content/drive/MyDrive/예제/vit-tiny.yaml\n"]}]},{"cell_type":"markdown","source":["### vit-tiny-custom.yaml 생성 스크립트\n","\n","- **용도**  \n","  사전학습 가중치 없이, 경량화된 ViT-Tiny 구조를 처음부터 학습할 때 사용\n","\n","- **주요 설정**  \n","  - `model.embed_dim = 128`, `model.depth = 6` (절반 깊이의 경량화 구조)  \n","  - `model.finetune_type = \"full\"` → 전체 블록 학습  \n","  - `optimizer` 및 `scheduler` 설정은 `vit-tiny.yaml`과 동일  \n","\n","- **사용 방법**  \n","  1. 스크립트를 실행하면 `/content/drive/MyDrive/예제/vit-tiny-custom.yaml` 파일이 생성됨  \n","  2. 데이터셋 경로 및 하이퍼파라미터를 프로젝트에 맞게 수정  \n"],"metadata":{"id":"mqCXnLHhiABD"}},{"cell_type":"code","source":["import yaml\n","from pathlib import Path\n","\n","# 1) 프로젝트 루트 경로\n","BASE = Path(r\"/content/drive/MyDrive/예제\")\n","\n","\n","# 2) yaml에 저장할 하이퍼파라미터 설정\n","vit_tiny_cfg = {\n","    \"epochs\": 10,\n","    \"batch_size\": 128, #128\n","    \"num_workers\": 2,\n","\n","    \"model\": {\n","        \"name\": \"vit_tiny_patch16_224\",\n","        \"patch_size\": 2,\n","        \"img_size\": 33,\n","        \"embed_dim\": 128, #128\n","        \"depth\":6, #6\n","        \"num_heads\": 2,\n","        \"mlp_ratio\": 2.0,\n","        \"qkv_bias\": True,\n","        \"drop_rate\": 0.1,\n","        \"attn_drop_rate\": 0.1,\n","        \"drop_path_rate\": 0.1,\n","        \"num_classes\": 2,\n","        \"finetune_type\": \"full\",       # \"full\" 또는 \"partial\"\n","        \"num_trainable_blocks\": 2\n","    },\n","\n","    \"optimizer\": {\n","        \"name\": \"AdamW\",\n","        \"lr\": 1e-3,\n","        \"weight_decay\": 0.01\n","    },\n","\n","    \"scheduler\": {\n","        \"name\": \"Linear\",                # 예: \"StepLR\", \"Cosine\", \"Linear\"\n","        \"total_epochs\": 10,\n","        \"warmup_epochs\": 2\n","    },\n","\n","    \"transform\": {\n","        \"resize\": True,\n","        \"random_rotation\": 0,\n","        \"horizontal_flip\": False,\n","        \"vertical_flip\": False,\n","        \"normalize\": True,\n","        \"mean\": [0.5, 0.5, 0.5],\n","        \"std\":  [0.5, 0.5, 0.5]\n","    },\n","\n","}\n","\n","# 3) YAML 파일 저장\n","outfile = BASE / \"vit-tiny-custom.yaml\"\n","with open(outfile, \"w\", encoding=\"utf-8\") as f:\n","    yaml.dump(vit_tiny_cfg, f, default_flow_style=False, sort_keys=False)\n","\n","print(f\"✅ YAML 설정 파일 저장 완료: {outfile}\")"],"metadata":{"id":"4i1waD-zhnKH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751039019730,"user_tz":-540,"elapsed":44,"user":{"displayName":"김다은","userId":"00408292954028336368"}},"outputId":"2f8361e9-a7cb-4774-f5da-08f57774b737"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ YAML 설정 파일 저장 완료: /content/drive/MyDrive/예제/vit-tiny-custom.yaml\n"]}]},{"cell_type":"markdown","source":["## 1. 환경 설정\n","\n","먼저 필요한 라이브러리를 설치 및 로드하고, **GPU가 사용 가능한지** 확인합니다.\n"],"metadata":{"id":"F9_nomi0SCBd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"c5n-Vv2gQMQv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751039021845,"user_tz":-540,"elapsed":7,"user":{"displayName":"김다은","userId":"00408292954028336368"}},"outputId":"4fcc8539-25c0-435e-b60e-7f28e8681305"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}],"source":["# 1) 핵심 라이브러리 불러오기\n","import os                   # 파일 시스템 경로 조작\n","import yaml                 # YAML 설정 파일 파싱\n","import pandas as pd         # CSV 읽기/쓰기\n","from PIL import Image       # 이미지 입출력\n","from sklearn.model_selection import train_test_split  # 데이터 분할\n","from tqdm import tqdm       # 진행률 표시\n","import torch                # PyTorch 메인 모듈\n","import torch.nn as nn       # 신경망 구성 요소\n","import torch.optim as optim # 최적화 알고리즘\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms  # 이미지 전처리\n","import timm                 # 사전학습된 Vision Transformer 로드\n","\n","# 2) 디바이스 설정: GPU 사용 권장\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Device: {device}\")  # GPU/CPU 정보 출력\n"]},{"cell_type":"markdown","source":["## 2. JetHeatmapDataset 클래스 구현\n","\n","이 셀에서는 **CSV** 파일(`labels.csv`)과 **이미지 디렉토리** 경로를 연결하여, 데이터셋을 **샘플 단위**로 로딩하는 클래스를 정의합니다.\n","\n","- **`__init__`** 메서드:\n","  - `csv_path`: 파일명과 레이블이 저장된 CSV 경로\n","  - `img_dir`: 이미지 파일들이 위치한 폴더 경로\n","  - `transform`: 전처리(transform) 파이프라인 객체\n","- **`__len__`** 메서드:\n","  - 데이터셋의 총 샘플 수 반환 (len(self.df))\n","- **`__getitem__`** 메서드:\n","  1. DataFrame에서 `fname, label` 추출\n","  2. `PIL.Image.open`으로 이미지 로드 후 RGB 변환\n","  3. 지정된 `transform` 적용 (예: Resize, ToTensor, Normalize)\n","  4. `(tensor_image, int(label))` 튜플 반환\n","\n","이 클래스를 통해 DataLoader가 **효율적으로 배치 단위**로 이미지를 로드할 수 있습니다."],"metadata":{"id":"kGnWNduESS7L"}},{"cell_type":"code","source":["class JetHeatmapDataset(Dataset):\n","    def __init__(self, csv_path, img_dir, transform=None):\n","        self.df = pd.read_csv(csv_path)       # (N, 2) shape: ['filename', 'label']\n","        self.img_dir = img_dir                # 이미지 디렉토리 경로\n","        self.transform = transform            # torchvision transforms\n","\n","    def __len__(self):\n","        return len(self.df)                  # 전체 샘플 수 반환\n","\n","    def __getitem__(self, idx):\n","        fname, label = self.df.iloc[idx]     # 파일명, 레이블 추출\n","        path = os.path.join(self.img_dir, fname)\n","        img = Image.open(path).convert('RGB') # RGB 이미지로 변환\n","        if self.transform:\n","            img = self.transform(img)        # 전처리 적용\n","        return img, int(label)              # 이미지 텐서, 레이블 반환\n"],"metadata":{"id":"CbY9BIZQQjGc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. 이미지 전처리(Transforms) 함수 정의\n","\n","`vit-tiny.yaml` 설정 파일의 `transform` 섹션에 정의된 옵션에 맞춰, **학습용(train)**과 **검증용(eval)** 파이프라인을 구성합니다.\n","\n","1. **Resize**: 입력 이미지를 `(img_size × img_size)`로 일괄 크기 조정(예: 33×33)  \n","2. **RandomHorizontalFlip / RandomVerticalFlip / RandomRotation**: 학습 시에만 적용하여 데이터 증강(data augmentation) 수행  \n","3. **ToTensor**: PIL 이미지를 PyTorch 텐서로 변환 (값 범위 [0,1])  \n","4. **Normalize**: 지정된 `mean`, `std`로 표준화 (값 분포 안정화)\n","\n","이러한 전처리를 통해 학습 안정성과 일반화 성능을 동시에 확보할 수 있습니다."],"metadata":{"id":"-ssil6aqSmYB"}},{"cell_type":"code","source":["def build_transforms(cfg, train=True):\n","    ops = []\n","    # 1) Resize (항상)\n","    if cfg['transform']['resize']:\n","        size = cfg['model']['img_size']\n","        ops.append(transforms.Resize((size, size)))\n","    # 2) 학습 시에만 데이터 증강\n","    if train:\n","        if cfg['transform']['horizontal_flip']:\n","            ops.append(transforms.RandomHorizontalFlip())\n","        if cfg['transform']['vertical_flip']:\n","            ops.append(transforms.RandomVerticalFlip())\n","        if cfg['transform']['random_rotation'] > 0:\n","            ops.append(transforms.RandomRotation(cfg['transform']['random_rotation']))\n","    # 3) 텐서 변환 및 정규화\n","    ops.append(transforms.ToTensor())\n","    if cfg['transform']['normalize']:\n","        ops.append(transforms.Normalize(cfg['transform']['mean'], cfg['transform']['std']))\n","    return transforms.Compose(ops)\n"],"metadata":{"id":"kJshzFBCQl70"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. 모델 로드 및 Fine-tuning 설정\n","\n","`build_model_components(cfg, device, ckpt_path)` 함수가 내부에서 수행하는 작업은 다음과 같습니다:\n","\n","1. **모델 생성**  \n","   - `timm.create_model(..., pretrained=False)` 로 ViT-Tiny 구조 초기화  \n","   - `img_size`, `patch_size`, `embed_dim`, `depth`, `num_heads`, `mlp_ratio`, `drop_rate`, `attn_drop_rate`, `drop_path_rate`, `num_classes` 등 아키텍처 하이퍼파라미터를 YAML 설정에서 불러옵니다.\n","\n","2. **체크포인트 로드**  \n","   - `torch.load(ckpt_path)` 로 저장된 `.pth` 파일을 읽어들입니다.  \n","   - `model_state_dict`, `state_dict` 키가 없을 때를 대비해 `ckpt.get(\"model_state_dict\", ckpt.get(\"state_dict\", ckpt))` 방식으로 안전하게 가중치를 불러와 `model.load_state_dict()` 합니다.\n","\n","3. **Fine-tuning 모드 적용**  \n","   - `finetune_type='full'`: 모델의 모든 파라미터를 업데이트 (Full fine-tuning)  \n","   - `finetune_type='partial'`: 앞쪽 블록을 freeze(동결)하고, 마지막 N개 블록 및 classification head만 학습 (Partial fine-tuning)\n","\n","4. **Optimizer & Loss 설정**  \n","   - `AdamW` 옵티마이저 (`lr`, `weight_decay`는 YAML 설정값 사용)  \n","   - `CrossEntropyLoss` (2-class 분류용)\n","\n","5. **학습률 스케줄러 (LambdaLR + Warmup)**  \n","   - 처음 `warmup_epochs` 동안 학습률을 선형 증가  \n","   - 이후 남은 에포크 동안 학습률을 선형 감소\n","\n","이 과정을 통해 반환된 `model`, `optimizer`, `criterion`, `scheduler`를 학습 루프에서 바로 사용할 수 있습니다.  \n"],"metadata":{"id":"89gR2D_pUY5w"}},{"cell_type":"code","source":["def build_model_components(cfg, device, ckpt_path):\n","    # 1) 모델 생성\n","    m = cfg[\"model\"]\n","    model = timm.create_model(\n","        m[\"name\"],\n","        pretrained=False,\n","        img_size=m[\"img_size\"],\n","        patch_size=m[\"patch_size\"],\n","        embed_dim=m[\"embed_dim\"],\n","        depth=m[\"depth\"],\n","        num_heads=m[\"num_heads\"],\n","        mlp_ratio=m[\"mlp_ratio\"],\n","        qkv_bias=m[\"qkv_bias\"],\n","        drop_rate=m[\"drop_rate\"],\n","        attn_drop_rate=m[\"attn_drop_rate\"],\n","        drop_path_rate=m[\"drop_path_rate\"],\n","        num_classes=m[\"num_classes\"]\n","    ).to(device)\n","\n","    # 2) 체크포인트 로드 (KeyError 방지용 fallback)\n","    ckpt = torch.load(ckpt_path, map_location=device)\n","    state_dict = ckpt.get(\"model_state_dict\",\n","                 ckpt.get(\"state_dict\",\n","                          ckpt))\n","    model.load_state_dict(state_dict)\n","\n","    # 3) partial fine-tuning 처리\n","    if m[\"finetune_type\"] == \"partial\":\n","        num_freeze = len(model.blocks) - m[\"num_trainable_blocks\"]\n","        for blk in model.blocks[:num_freeze]:\n","            for p in blk.parameters():\n","                p.requires_grad = False\n","\n","    # 4) 옵티마이저 & 손실함수\n","    optimizer = optim.AdamW(\n","        model.parameters(),\n","        lr=cfg[\"optimizer\"][\"lr\"],\n","        weight_decay=cfg[\"optimizer\"][\"weight_decay\"]\n","    )\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # 5) 스케줄러 (LambdaLR with warmup)\n","\n","    total_epochs  = cfg[\"epochs\"]\n","    warmup_epochs = cfg[\"scheduler\"][\"warmup_epochs\"]\n","    def lr_lambda(epoch):\n","        if epoch < warmup_epochs:\n","            return (epoch + 1) / warmup_epochs\n","        return max(0.0, (total_epochs - epoch) / (total_epochs - warmup_epochs))\n","    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n","\n","    model.train()\n","    return model, optimizer, criterion, scheduler\n"],"metadata":{"id":"rgHswMv4-kM7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <Transformer Encoder 블록>\n","\n","이번 셀에서는 **ViT-Tiny** 모델의 **Transformer Encoder** 블록을 설명합니다.\n","비록 코드로 구현하지는 않았지만 입력 패치 임베딩이 어떻게 처리되고, 각각의 블록이 어떤 순서로 구성되는지 확인합니다.\n","\n","---\n","\n","### 하나의 Encoder 블록 구성\n","\n","1. **LayerNorm**\n","   - **역할**: 입력 임베딩에 대해 정규화 수행 → 학습 안정성 및 빠른 수렴\n","\n","2. **Multi-Head Self-Attention (MHSA)**\n","   - **역할**: 입력 패치 간 상호작용을 통해 전역적인 특징(feature) 추출\n","   - **파라미터**:\n","     - `embed_dim` → 임베딩 차원 (예: 192)\n","     - `num_heads` → 어텐션 헤드 수 (예: 2)\n","     - `qkv_bias` → Q/K/V 생성 시 바이어스 사용 여부\n","\n","3. **DropPath (Stochastic Depth)**\n","   - **역할**: 블록 단위 드롭아웃 → 과적합 방지, 모델 일반화 향상\n","   - **확률**: `drop_path_rate`에 따라 선형적으로 증가\n","\n","4. **Residual 연결 (Add)**\n","   - **역할**: 블록 입력과 MHSA+DropPath 출력의 합을 계산 → 기울기 흐름 개선\n","\n","5. **LayerNorm**\n","   - **역할**: MLP 블록 입력 정규화\n","\n","6. **MLP (Feed-Forward Network)**\n","   - **구성**:\n","     - `Linear(embed_dim, embed_dim * mlp_ratio)` → 차원 확장\n","     - `GELU` 활성화 → 비선형성 추가\n","     - `Linear(embed_dim * mlp_ratio, embed_dim)` → 원래 차원 복원\n","\n","7. **DropPath** 및 **Residual 연결**\n","   - MLP 출력에도 DropPath 적용 후 입력과 더함\n","\n","---\n","\n","### 블록별 입출력 크기 변화\n","\n","| 블록 번호 | 입력 패치 수 | 임베딩 차원 | 출력 형태        |\n","|:---------:|:-----------:|:----------:|:----------------:|\n","| Block 1   | 65 (1 cls + 64) | 192       | (65, 192)        |\n","| Block 2 ~ 12 | 65         | 192       | (65, 192)        |\n","\n","- **cls 토큰 위치**: 첫 번째 패치에 해당하는 벡터 (분류용)  \n","- **패치 토큰 위치**: 나머지 64개 패치 벡터 (특징 표현)\n","\n","---\n","\n","### 전체 Transformer Encoder 쌓기\n","\n","1. **입력**: 33×33 이미지를 16×16 패치로 분할 → 3×(16×16) 색상\n","2. **패치 임베딩**: `Linear(3*patch_size*patch_size, embed_dim)` → 192차원 벡터\n","3. **클래스 토큰** 추가 및 **위치 임베딩** 적용\n","4. **Encoder 블록** 12회 반복 (ViT-Tiny depth = 12)\n","5. **LayerNorm** → 최종 `cls` 벡터 추출\n","6. **MLP 헤드**: `Linear(embed_dim, num_classes)` → 클래스 로짓"],"metadata":{"id":"iWhD13ViUcwD"}},{"cell_type":"markdown","source":["## 5. 학습 및 검증 루프 정의\n","\n","### 손실 함수 & 옵티마이저\n","- **Criterion**: `nn.CrossEntropyLoss()` → raw logits을 softmax + log로 변환해 클래스 간 차이 측정  \n","- **Optimizer**: `optim.AdamW()` → weight decay가 적용된 Adam 알고리즘\n","\n","### 함수 구조\n","- **train_epoch**(학습 모드)\n","  1. `model.train()` 활성화 → Dropout/BatchNorm 학습 모드\n","  2. 배치별 Forward → Loss 계산 → Backward → Optimizer Step\n","  3. 배치 손실·정확도 누적 → 에포크 평균 반환\n","\n","- **eval_epoch**(검증 모드)\n","  1. `model.eval()` & `torch.no_grad()` → 기울기 비활성화\n","  2. 배치별 Forward → Loss·정확도 누적\n","  3. 에포크 평균 반환"],"metadata":{"id":"jxEmMjJRWFQH"}},{"cell_type":"code","source":["def train_epoch(model, loader, criterion, optimizer, device):\n","    model.train()\n","    total_loss, total_correct = 0.0, 0\n","    for x, y in tqdm(loader, desc=\"Training\", leave=True):\n","        x, y = x.to(device), y.to(device)\n","        optimizer.zero_grad()\n","        out = model(x)\n","        loss = criterion(out, y)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * x.size(0)\n","        total_correct += (out.argmax(1) == y).sum().item()\n","    n = len(loader.dataset)\n","    return total_loss / n, total_correct / n\n","\n","\n","def eval_epoch(model, loader, criterion, device):\n","    model.eval()\n","    total_loss, total_correct = 0.0, 0\n","    with torch.no_grad():\n","        for x, y in tqdm(loader, desc=\"Validating\", leave=True):\n","            x, y = x.to(device), y.to(device)\n","            out = model(x)\n","            loss = criterion(out, y)\n","            total_loss += loss.item() * x.size(0)\n","            total_correct += (out.argmax(1) == y).sum().item()\n","    n = len(loader.dataset)\n","    return total_loss / n, total_correct / n"],"metadata":{"id":"b9IyMUfbQpyk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Main 함수: 전체 파이프라인 실행\n","\n","다음 코드는 한 번에 전체 학습 과정을 실행하기 위한 주요 단계들을 설명합니다.\n","\n","1. **설정 로드 & 디바이스 선택**  \n","   - `vit-tiny-custom.yaml` 파일에서 하이퍼파라미터(`epochs`, `batch_size`, `optimizer` 등)를 불러옵니다.  \n","   - `torch.device(\"cuda\" if available else \"cpu\")`를 통해 GPU 혹은 CPU를 선택합니다.  \n","   - 학습된 체크포인트를 저장·불러올 경로(`ckpt_path`)를 지정합니다.\n","\n","2. **모델·옵티마이저·스케줄러·손실함수 초기화**  \n","   - `build_model_components(cfg, device, ckpt_path)` 함수를 호출하여  \n","     - ViT-Tiny 모델을 생성하고 (`img_size`, `patch_size`, `embed_dim`, `depth` 등 YAML 설정 반영)  \n","     - 학습된 가중치를 로드하며  \n","     - Full/Partial fine-tuning 모드를 적용하고  \n","     - `AdamW` 옵티마이저, `CrossEntropyLoss`, LambdaLR 스케줄러를 반환받습니다.\n","\n","3. **데이터 분할 및 DataLoader 생성**  \n","   - `labels.csv`를 읽어와 80:20 비율로 stratified split(`train_labels.csv`, `val_labels.csv`)합니다.  \n","   - `JetHeatmapDataset` 클래스와 `build_transforms(cfg, train=True/False)`를 이용해 학습 및 검증용 데이터셋을 준비합니다.  \n","   - `DataLoader`에 `batch_size`, `shuffle`, `num_workers` 설정을 반영해 `train_loader`, `val_loader`를 생성합니다.\n","\n","4. **학습 루프 실행**  \n","   - 지정된 `epochs`만큼 반복하며:  \n","     1. `train_epoch(...)` 호출로 학습 손실과 정확도를 계산  \n","     2. `eval_epoch(...)` 호출로 검증 손실과 정확도를 계산  \n","     3. 터미널에  \n","        ```\n","        [Epoch/Total] Train Loss: xx.xx, Acc: xx.xx | Val Loss: xx.xx, Acc: xx.xx\n","        ```  \n","        형태로 출력  \n","     4. `scheduler.step()`로 학습률을 업데이트  \n","     5. 매 에포크마다 `torch.save(...)`로 `model_state_dict`와 `optimizer_state_dict`를 `ckpt_path`에 덮어쓰며 저장\n","\n","5. **학습 완료**  \n","   - 모든 에포크가 종료된 후 `\"✅ Training complete.\"` 메시지를 출력하여 학습 과정을 종료합니다.\n"],"metadata":{"id":"S5w05llVUf23"}},{"cell_type":"code","source":["# ─────────────────────────────────────\n","# 설정 & 디바이스\n","# ─────────────────────────────────────\n","cfg_path  = \"/content/drive/MyDrive/예제/vit-tiny.yaml\"\n","ckpt_path = \"/content/drive/MyDrive/예제/checkpoint/timm_vit_finetuned.pth\"\n","device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","cfg       = yaml.safe_load(open(cfg_path, \"r\"))\n","\n","# ─────────────────────────────────────\n","# 모델·옵티·스케줄러·손실 설정\n","# ─────────────────────────────────────\n","model, optimizer, criterion, scheduler = build_model_components(cfg, device, ckpt_path)\n","\n","# ─────────────────────────────────────\n","# DataLoader 준비\n","# ─────────────────────────────────────\n","base_data = \"/content/drive/MyDrive/예제/dataset/MC\"\n","labels_csv = os.path.join(base_data, \"labels.csv\")\n","df = pd.read_csv(labels_csv)\n","\n","# train/val split\n","tr, va = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n","train_csv = os.path.join(base_data, \"train_labels.csv\")\n","val_csv   = os.path.join(base_data, \"val_labels.csv\")\n","tr.to_csv(train_csv, index=False)\n","va.to_csv(val_csv, index=False)\n","\n","# Dataset & DataLoader\n","train_ds = JetHeatmapDataset(train_csv, base_data, transform=build_transforms(cfg, train=True))\n","val_ds   = JetHeatmapDataset(val_csv,   base_data, transform=build_transforms(cfg, train=False))\n","\n","train_loader = DataLoader(\n","    train_ds,\n","    batch_size=cfg[\"batch_size\"],\n","    shuffle=True,\n","    num_workers=cfg[\"num_workers\"]\n",")\n","val_loader = DataLoader(\n","    val_ds,\n","    batch_size=cfg[\"batch_size\"],\n","    shuffle=False,\n","    num_workers=cfg[\"num_workers\"]\n",")\n","\n","# ─────────────────────────────────────\n","# 학습 루프\n","# ─────────────────────────────────────\n","for epoch in range(1, cfg[\"epochs\"] + 1):\n","    tloss, tacc = train_epoch(model, train_loader, criterion, optimizer, device)\n","    vloss, vacc = eval_epoch(model, val_loader,   criterion, device)\n","\n","    print(f\"[{epoch}/{cfg['epochs']}] Train {tloss:.4f}/{tacc:.4f} | Val {vloss:.4f}/{vacc:.4f}\")\n","\n","    scheduler.step()\n","    torch.save({\n","        \"epoch\": epoch,\n","        \"model_state_dict\": model.state_dict(),\n","        \"optimizer_state_dict\": optimizer.state_dict()\n","    }, ckpt_path)\n","\n","print(\"✅ Training complete.\")\n","\n"],"metadata":{"id":"4GXqoS6jQrgp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751040437690,"user_tz":-540,"elapsed":83102,"user":{"displayName":"김다은","userId":"00408292954028336368"}},"outputId":"4776e2ab-7f50-43ea-f461-1205cde3cb8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[1/10] Train 0.6957/0.4763 | Val 0.6928/0.5100\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[2/10] Train 0.6943/0.4975 | Val 0.6954/0.5100\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[3/10] Train 0.6980/0.5112 | Val 0.6943/0.4900\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[4/10] Train 0.6983/0.4913 | Val 0.6926/0.5250\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[5/10] Train 0.6903/0.5262 | Val 0.6736/0.6550\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[6/10] Train 0.6724/0.6375 | Val 0.6305/0.6850\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[7/10] Train 0.6683/0.6038 | Val 0.6319/0.6850\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[8/10] Train 0.6488/0.6325 | Val 0.5955/0.7050\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[9/10] Train 0.6301/0.6587 | Val 0.5691/0.6900\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[10/10] Train 0.6220/0.6525 | Val 0.5921/0.7050\n","✅ Training complete.\n"]}]}]}